#!/usr/bin/env python3
"""
Theme Insight Generator
Aggregates comments by category (module), module name (submodule), and sentiment.
Uses Claude Haiku 4.5 to map comments to candidate themes.
Uses Claude Sonnet 4.5 to reduce and generate insights.
"""

import os
import pandas as pd
import json
import time
from datetime import datetime
from typing import Dict, List, Tuple
from collections import defaultdict
from tqdm import tqdm
import yaml
import numpy as np

try:
    from anthropic import Anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    print("âš ï¸ anthropic not installed. Install with: pip install anthropic")


class ThemeInsightGenerator:
    def __init__(self, processed_csv_file: str, config_file: str = "themes_config_new.yaml", 
                 api_key: str = None, output_dir: str = None):
        """
        Initialize the Theme Insight Generator
        
        Args:
            processed_csv_file: Path to the processed comments CSV file
            config_file: Path to the theme configuration YAML file
            api_key: Anthropic API key (or use ANTHROPIC_API_KEY env var)
            output_dir: Output directory for results
        """
        self.processed_csv_file = processed_csv_file
        self.config_file = config_file
        self.output_dir = output_dir or ""
        if self.output_dir and not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir, exist_ok=True)
        
        # Anthropic setup
        if not ANTHROPIC_AVAILABLE:
            raise ImportError("anthropic package is required. Install with: pip install anthropic")
        
        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY", "")
        if not self.api_key:
            # Try to get from Browser API key format if provided
            browser_key = os.environ.get("BROWSER_API_KEY", "")
            if browser_key:
                self.api_key = browser_key
            else:
                raise ValueError("Anthropic API key is required. Provide via api_key parameter, ANTHROPIC_API_KEY, or BROWSER_API_KEY env var.")
        
        self.anthropic_client = Anthropic(api_key=self.api_key)
        self.haiku_model = "claude-haiku-4-5-20251001"  # Claude Haiku 4.5
        self.sonnet_model = "claude-sonnet-4-5-20250929"  # Claude Sonnet 4.5
        
        # Data storage
        self.df = None
        self.config = None
        self.grouped_comments = {}
        self.candidate_themes = {}
        self.insights = {}
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.limit_groups = None  # Limit number of groups to process (int) or filter by pattern (str) (for testing)
    
    def _fix_json_text(self, text: str) -> str:
        """Try to fix common JSON formatting issues with more aggressive fixes"""
        import re
        import json
        
        original_text = text
        
        # Step 1: Remove markdown code blocks if still present
        if text.startswith("```"):
            text = text.split("```")[1]
            if text.startswith("json"):
                text = text[4:]
        text = text.strip()
        
        # Step 2: Remove trailing commas before closing brackets/braces (multiple passes)
        for _ in range(5):  # More passes for nested cases
            text = re.sub(r',(\s*[}\]])', r'\1', text)
        
        # Step 3: Try to extract JSON object/array if text contains other content
        first_brace = text.find('{')
        first_bracket = text.find('[')
        
        if first_brace >= 0 and (first_bracket < 0 or first_brace < first_bracket):
            start = first_brace
            # Find matching closing brace, handling strings properly
            depth = 0
            in_string = False
            escape_next = False
            for i in range(start, len(text)):
                if escape_next:
                    escape_next = False
                    continue
                if text[i] == '\\':
                    escape_next = True
                    continue
                if text[i] == '"' and not escape_next:
                    in_string = not in_string
                    continue
                if not in_string:
                    if text[i] == '{':
                        depth += 1
                    elif text[i] == '}':
                        depth -= 1
                        if depth == 0:
                            text = text[start:i+1]
                            break
        elif first_bracket >= 0:
            start = first_bracket
            # Find matching closing bracket
            depth = 0
            in_string = False
            escape_next = False
            for i in range(start, len(text)):
                if escape_next:
                    escape_next = False
                    continue
                if text[i] == '\\':
                    escape_next = True
                    continue
                if text[i] == '"' and not escape_next:
                    in_string = not in_string
                    continue
                if not in_string:
                    if text[i] == '[':
                        depth += 1
                    elif text[i] == ']':
                        depth -= 1
                        if depth == 0:
                            text = text[start:i+1]
                            break
        
        # Step 4: Remove control characters except newlines and tabs
        text = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F]', '', text)
        
        # Step 5: Try to fix common JSON issues
        try:
            json.loads(text)
            return text.strip()
        except json.JSONDecodeError as e:
            # More aggressive fixes
            error_pos = getattr(e, 'pos', None)
            
            # Fix unescaped newlines in strings (replace actual newlines with \n)
            # This is a common issue when comments contain newlines
            lines = text.split('\n')
            fixed_lines = []
            in_string = False
            escape_next = False
            
            for line in lines:
                fixed_line = ""
                i = 0
                while i < len(line):
                    char = line[i]
                    
                    if escape_next:
                        fixed_line += char
                        escape_next = False
                        i += 1
                        continue
                    
                    if char == '\\':
                        fixed_line += char
                        escape_next = True
                        i += 1
                        continue
                    
                    if char == '"':
                        in_string = not in_string
                        fixed_line += char
                        i += 1
                        continue
                    
                    # If we're in a string and hit a newline (shouldn't happen in valid JSON)
                    # This means the string wasn't properly closed or escaped
                    if in_string and char == '\n':
                        # Replace with \n escape sequence
                        fixed_line += '\\n'
                        i += 1
                        continue
                    
                    fixed_line += char
                    i += 1
                
                fixed_lines.append(fixed_line)
            
            text = '\n'.join(fixed_lines)
            
            # Step 6: Try to fix unterminated strings - more aggressive approach
            # If error mentions "Unterminated string", try to close it
            error_msg = str(e)
            if 'Unterminated string' in error_msg or 'Expecting' in error_msg:
                if error_pos and error_pos < len(text):
                    # Strategy 1: Find the unterminated string and close it
                    # Look backwards from error_pos to find the opening quote
                    start_pos = error_pos
                    quote_count = 0
                    while start_pos > 0:
                        if text[start_pos] == '"':
                            # Check if it's escaped
                            if start_pos == 0 or text[start_pos-1] != '\\':
                                quote_count += 1
                                if quote_count == 1:  # Found the opening quote
                                    break
                        start_pos -= 1
                    
                    if start_pos >= 0 and text[start_pos] == '"':
                        # Found opening quote, now find where to close
                        # Look forward for natural closing points
                        insert_pos = None
                        
                        # Strategy A: Look for next comma, brace, or bracket (likely end of value)
                        for i in range(error_pos, min(error_pos + 300, len(text))):
                            if i >= len(text):
                                break
                            char = text[i]
                            # If we hit a comma or closing brace/bracket, likely end of value
                            if char in [',', '}', ']']:
                                # Check if previous char is not escaped quote
                                if i > 0 and text[i-1] != '"':
                                    insert_pos = i
                                    break
                            # If we hit a newline followed by spaces and then quote/brace/comma
                            elif char == '\n':
                                # Look ahead to see if we're at a structure boundary
                                ahead = text[i+1:min(i+50, len(text))].strip()
                                if ahead.startswith('"') or ahead.startswith(',') or ahead.startswith('}') or ahead.startswith(']'):
                                    insert_pos = i
                                    break
                        
                        # Strategy B: If no natural end found, close at error_pos + reasonable distance
                        if insert_pos is None:
                            # Look for end of line or reasonable stopping point
                            for i in range(error_pos, min(error_pos + 200, len(text))):
                                if i >= len(text):
                                    insert_pos = len(text)
                                    break
                                if text[i] == '\n':
                                    # Check if next line starts a new JSON structure
                                    next_line = text[i+1:min(i+30, len(text))].strip()
                                    if next_line.startswith('"') or next_line.startswith('}') or next_line.startswith(']'):
                                        insert_pos = i
                                        break
                            
                            if insert_pos is None:
                                # Last resort: close at error_pos + 100 chars
                                insert_pos = min(error_pos + 100, len(text))
                        
                        # Insert closing quote
                        if insert_pos is not None:
                            text = text[:insert_pos] + '"' + text[insert_pos:]
                    
                    # Strategy 2: If still failing, try to extract valid JSON up to error
                    try:
                        json.loads(text)
                        return text.strip()
                    except json.JSONDecodeError as e2:
                        # Try to extract partial valid JSON
                        if error_pos and error_pos < len(text):
                            # Try to find the last complete JSON structure before error
                            # Look for last complete object/array
                            last_brace = text.rfind('}', 0, error_pos)
                            last_bracket = text.rfind(']', 0, error_pos)
                            
                            if last_brace > 0 or last_bracket > 0:
                                # Try to extract up to the last complete structure
                                extract_pos = max(last_brace, last_bracket)
                                if extract_pos > 0:
                                    # Find matching opening
                                    if text[extract_pos] == '}':
                                        # Find matching {
                                        depth = 1
                                        for i in range(extract_pos - 1, -1, -1):
                                            if text[i] == '}':
                                                depth += 1
                                            elif text[i] == '{':
                                                depth -= 1
                                                if depth == 0:
                                                    try:
                                                        partial_json = text[i:extract_pos+1]
                                                        json.loads(partial_json)
                                                        # If partial JSON is valid, try to reconstruct
                                                        # For now, just try the fixed version
                                                        pass
                                                    except:
                                                        pass
                                                    break
                            
                            # Final attempt: try the fixed text
                            try:
                                json.loads(text)
                                return text.strip()
                            except:
                                pass
            
            # Final attempt: return the text and let caller handle
            return text.strip()
    
    def load_config(self):
        """Load theme configuration from YAML"""
        print(f"ğŸ“‹ Loading theme configuration from {self.config_file}...")
        with open(self.config_file, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        print(f"   âœ… Configuration loaded")
    
    def load_data(self):
        """Load processed comments from CSV"""
        print(f"\nğŸ“‚ Loading processed comments from {self.processed_csv_file}...")
        self.df = pd.read_csv(self.processed_csv_file, lineterminator="\n")
        
        # Filter out duplicates if _is_duplicate column exists
        if '_is_duplicate' in self.df.columns:
            before_count = len(self.df)
            self.df = self.df[~self.df['_is_duplicate']].reset_index(drop=True)
            after_count = len(self.df)
            print(f"   âœ… Loaded {after_count} comments (removed {before_count - after_count} duplicates)")
        else:
            print(f"   âœ… Loaded {len(self.df)} comments")
        
        # Find text column
        text_cols = ["comment_text", "_text_original", "text", "content", "comment", "body"]
        self.text_col = None
        for col in text_cols:
            if col in self.df.columns:
                self.text_col = col
                break
        
        if not self.text_col:
            raise ValueError("Could not find text column in CSV file")
        
        print(f"   ğŸ“ Using text column: {self.text_col}")
    
    def aggregate_comments(self):
        """Aggregate comments by module (category) and sentiment only (no submodule)"""
        print(f"\nğŸ“Š Aggregating comments by category and sentiment (no submodule)...")
        
        # Fill NaN values with defaults
        if '_sentiment' in self.df.columns:
            self.df['_sentiment'] = self.df['_sentiment'].fillna('none')
        if '_sentiment_name' in self.df.columns:
            self.df['_sentiment_name'] = self.df['_sentiment_name'].fillna('N/A')
        
        # Group by module and sentiment only (no submodule)
        grouping_cols = ['_module', '_module_name', '_sentiment', '_sentiment_name']
        
        # Check which columns exist
        available_cols = [col for col in grouping_cols if col in self.df.columns]
        
        if not available_cols:
            raise ValueError("Required classification columns not found in CSV file")
        
        print(f"   Grouping by: {', '.join(available_cols)}")
        
        # Group the data
        grouped = self.df.groupby(available_cols, dropna=False)
        
        self.grouped_comments = {}
        
        for group_key, group_df in grouped:
            if isinstance(group_key, tuple):
                # Extract values, handling NaN
                values = []
                for val in group_key:
                    if pd.isna(val):
                        values.append('N/A')
                    else:
                        values.append(str(val))
                
                # Create a key from the group (module + sentiment only)
                if len(values) >= 4:
                    module_id, module_name, sentiment, sentiment_name = values[:4]
                elif len(values) >= 2:
                    module_id, module_name = values[:2]
                    sentiment = values[2] if len(values) > 2 else 'none'
                    sentiment_name = values[3] if len(values) > 3 else 'N/A'
                else:
                    continue
            else:
                continue
            
            # Skip if module_name is missing
            if module_name == 'N/A' or not module_name:
                continue
            
            # Create a unique key for this group (module + sentiment only)
            group_key_str = f"{module_name}|{sentiment_name}"
            
            # Get comments for this group (filter out empty comments)
            comments = [str(c) for c in group_df[self.text_col].tolist() if pd.notna(c) and str(c).strip()]
            
            if not comments:
                continue
            
            # Store group information
            self.grouped_comments[group_key_str] = {
                'module_id': module_id,
                'module_name': module_name,
                'sentiment': sentiment,
                'sentiment_name': sentiment_name,
                'comments': comments,
                'comment_count': len(comments),
                'like_count': int(group_df['like_count'].sum()) if 'like_count' in group_df.columns else 0,
                'avg_likes': float(group_df['like_count'].mean()) if 'like_count' in group_df.columns else 0.0
            }
        
        print(f"   âœ… Aggregated into {len(self.grouped_comments)} groups")
        
        # Print summary
        print(f"\n   All groups by comment count:")
        sorted_groups = sorted(self.grouped_comments.items(), 
                             key=lambda x: x[1]['comment_count'], 
                             reverse=True)
        for group_key, group_data in sorted_groups:
            print(f"      {group_key}: {group_data['comment_count']} comments")
    
    def map_candidate_themes_haiku(self, comments: List[str], module_name: str, 
                                   sentiment_name: str, 
                                   max_retries: int = 5) -> Dict:
        """
        Use Claude Haiku 4.5 to map comments to candidate themes
        
        Args:
            comments: List of comments for this group
            module_name: Module name (category)
            sentiment_name: Sentiment name
            max_retries: Maximum number of retries for API calls
        
        Returns:
            Dictionary with candidate themes
        """
        # Limit number of comments to process (to avoid token limits)
        # Take a sample if too many comments
        # Increased from 100 to 150 to allow more key_examples
        max_comments_per_batch = 150
        if len(comments) > max_comments_per_batch:
            # Sample comments, prioritizing longer ones
            comments_sorted = sorted(comments, key=len, reverse=True)
            comments_to_process = comments_sorted[:max_comments_per_batch]
            comments_sample_info = f" (sampled {max_comments_per_batch} from {len(comments)})"
        else:
            comments_to_process = comments
            comments_sample_info = ""
        
        # Prepare comments text - æ¸…æ´—è¯„è®ºä»¥é¿å…JSONè§£æé—®é¢˜
        cleaned_comments = []
        for c in comments_to_process:
            if not c or not isinstance(c, str):
                continue
            # åŸºæœ¬æ¸…æ´—ï¼šç§»é™¤æ§åˆ¶å­—ç¬¦ï¼Œé™åˆ¶é•¿åº¦
            import re
            cleaned = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F]', '', str(c))
            if len(cleaned) > 500:
                last_space = cleaned.rfind(' ', 0, 500)
                if last_space > 400:
                    cleaned = cleaned[:last_space] + "..."
                else:
                    cleaned = cleaned[:500] + "..."
            cleaned_comments.append(cleaned.strip())
        
        comments_text = "\n\n".join([f"{i+1}. {comment}" for i, comment in enumerate(cleaned_comments)])
        
        for attempt in range(max_retries):
            try:
                prompt = f"""åˆ†æä»¥ä¸‹å…³äº"{module_name}"ç±»åˆ«ã€æƒ…æ„Ÿä¸º"{sentiment_name}"çš„è¯„è®ºã€‚

ä½ çš„ä»»åŠ¡æ˜¯è¯†åˆ«è¿™äº›è¯„è®ºä¸­å‡ºç°çš„ä¸»è¦ä¸»é¢˜ï¼ˆè¯é¢˜ã€å…³æ³¨ç‚¹æˆ–æ¨¡å¼ï¼‰ã€‚

è¯„è®ºåˆ—è¡¨ï¼š
{comments_text}

è¦æ±‚ï¼š
1. è¯†åˆ«3-10ä¸ªä¸åŒçš„å€™é€‰ä¸»é¢˜ï¼Œä»£è¡¨è¯„è®ºä¸­çš„ä¸»è¦è¯é¢˜ã€å…³æ³¨ç‚¹æˆ–æ¨¡å¼
2. å¯¹äºæ¯ä¸ªä¸»é¢˜ï¼Œæä¾›ï¼š
   - ä¸»é¢˜åç§°ï¼ˆç®€æ´ã€æè¿°æ€§ï¼‰
   - æè¿°ï¼ˆè¯¥ä¸»é¢˜æ˜¯å…³äºä»€ä¹ˆçš„ï¼‰
   - å…³é”®ç¤ºä¾‹ï¼ˆ5-10æ¡ä»£è¡¨æ€§çš„è¯„è®ºåŸæ–‡ï¼Œä¿æŒåŸæ ·ã€‚å¦‚æœè¯¥ä¸»é¢˜ç›¸å…³è¯„è®ºå¾ˆå¤šï¼Œå¯ä»¥åŒ…å«æ›´å¤šï¼Œæœ€å¤š15æ¡ï¼‰
   - é¢‘ç‡æŒ‡æ ‡ï¼ˆè¯¥ä¸»é¢˜å‡ºç°çš„é¢‘ç‡ï¼š"éå¸¸å¸¸è§"ã€"å¸¸è§"ã€"å¶å°”"ã€"ç½•è§"ï¼‰
   - è¯„è®ºæ•°é‡ï¼ˆæåˆ°è¯¥ä¸»é¢˜çš„è¯„è®ºæ•°é‡ï¼‰

3. å…³æ³¨å¯æ“ä½œçš„æ´å¯Ÿ - èƒ½å¤Ÿä¸ºäº§å“å†³ç­–æä¾›ä¿¡æ¯çš„ä¸»é¢˜
4. å°†ç›¸ä¼¼çš„è¯„è®ºå½’ç±»åˆ°åŒä¸€ä¸»é¢˜ä¸‹
5. è¦å…·ä½“å’Œå…·ä½“ - é¿å…æ¨¡ç³Šçš„ä¸»é¢˜
6. å…³é”®ç¤ºä¾‹å¿…é¡»æ˜¯è¯„è®ºçš„åŸæ–‡ï¼Œä¸è¦æ”¹å†™æˆ–æ€»ç»“

è¾“å‡ºæ ¼å¼ï¼ˆä»…JSONï¼‰ï¼š
{{
  "candidate_themes": [
    {{
      "theme_name": "ä¸»é¢˜åç§°",
      "description": "è¯¦ç»†æè¿°",
      "key_examples": ["è¯„è®ºåŸæ–‡1", "è¯„è®ºåŸæ–‡2", "è¯„è®ºåŸæ–‡3", "è¯„è®ºåŸæ–‡4", "è¯„è®ºåŸæ–‡5", "...æ›´å¤šè¯„è®º"],
      "frequency": "éå¸¸å¸¸è§|å¸¸è§|å¶å°”|ç½•è§",
      "comment_count": æåˆ°è¯¥ä¸»é¢˜çš„è¯„è®ºæ•°é‡
    }}
  ],
  "summary": "è¯„è®ºçš„æ•´ä½“æ‘˜è¦ï¼ˆ2-3å¥è¯ï¼‰"
}}

é‡è¦è¦æ±‚ï¼š
1. ä»…è¿”å›æœ‰æ•ˆçš„JSONï¼Œä¸è¦markdownä»£ç å—ï¼Œä¸è¦è§£é‡Šæ–‡å­—
2. ç¡®ä¿æ‰€æœ‰å­—ç¬¦ä¸²ä¸­çš„å¼•å·éƒ½æ­£ç¡®è½¬ä¹‰ï¼ˆä½¿ç”¨\\"ï¼‰
3. ç¡®ä¿æ‰€æœ‰JSONè¯­æ³•æ­£ç¡®ï¼ˆæ²¡æœ‰å°¾éšé€—å·ï¼Œæ‹¬å·åŒ¹é…ç­‰ï¼‰
4. å¦‚æœè¯„è®ºä¸­åŒ…å«å¼•å·ã€æ¢è¡Œç¬¦ç­‰ç‰¹æ®Šå­—ç¬¦ï¼Œå¿…é¡»æ­£ç¡®è½¬ä¹‰
5. ç¡®ä¿JSONå¯ä»¥ç«‹å³è¢«è§£æï¼Œæ— éœ€ä»»ä½•ä¿®æ”¹"""

                response = self.anthropic_client.messages.create(
                    model=self.haiku_model,
                    max_tokens=8192,  # Large limit to allow full responses
                    messages=[
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ]
                )
                
                result_text = response.content[0].text.strip()
                
                # Remove markdown code blocks if present
                if result_text.startswith("```"):
                    result_text = result_text.split("```")[1]
                    if result_text.startswith("json"):
                        result_text = result_text[4:]
                result_text = result_text.strip()
                
                # Try to fix common JSON issues
                result_text = self._fix_json_text(result_text)
                
                # Parse JSON
                try:
                    result = json.loads(result_text)
                    # Add metadata
                    result['total_comments'] = len(comments)
                    result['comments_analyzed'] = len(comments_to_process)
                    result['sample_info'] = comments_sample_info
                    # Ensure all themes have key_examples with actual comment text
                    for theme in result.get('candidate_themes', []):
                        if 'key_examples' not in theme or not theme['key_examples']:
                            theme['key_examples'] = []
                    return result
                except json.JSONDecodeError as e:
                    error_msg = str(e)
                    error_pos = getattr(e, 'pos', None)
                    print(f"   âš ï¸ JSONè§£æé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {error_msg[:150]}")
                    if error_pos:
                        print(f"      é”™è¯¯ä½ç½®: {error_pos}, ä¸Šä¸‹æ–‡: {result_text[max(0, error_pos-50):error_pos+50]}")
                    
                    if attempt < max_retries - 1:
                        # Try more aggressive JSON fixing
                        print(f"   ğŸ”„ å°è¯•æ›´æ¿€è¿›çš„JSONä¿®å¤...")
                        result_text = self._fix_json_text(result_text)
                        
                        # Try parsing again with fixed text
                        try:
                            result = json.loads(result_text)
                            # If successful, continue with the fixed result
                            print(f"   âœ… JSONä¿®å¤æˆåŠŸï¼")
                            # Ensure all insights have supporting_comments
                            for insight in result.get('key_insights', []):
                                if 'supporting_comments' not in insight:
                                    insight['supporting_comments'] = []
                            for theme in result.get('priority_themes', []):
                                if 'supporting_comments' not in theme:
                                    theme['supporting_comments'] = []
                            if 'sentiment_analysis' in result and 'supporting_comments' not in result['sentiment_analysis']:
                                result['sentiment_analysis']['supporting_comments'] = []
                            return result
                        except json.JSONDecodeError:
                            # Still failing, retry with new API call
                            print(f"   ğŸ”„ JSONä¿®å¤å¤±è´¥ï¼Œé‡è¯•APIè°ƒç”¨ (ç­‰å¾… {2 ** attempt} ç§’)...")
                            # Save problematic response for debugging
                            if attempt == 1:
                                debug_file = f"debug_json_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                                try:
                                    with open(debug_file, 'w', encoding='utf-8') as f:
                                        f.write(f"Error: {error_msg}\n")
                                        f.write(f"Error position: {error_pos}\n")
                                        f.write(f"Response length: {len(result_text)}\n")
                                        f.write(f"Context around error: {result_text[max(0, error_pos-100):error_pos+100]}\n")
                                        f.write(f"\nFull response:\n{result_text}\n")
                                    print(f"   ğŸ’¾ å·²ä¿å­˜è°ƒè¯•ä¿¡æ¯åˆ°: {debug_file}")
                                except:
                                    pass
                            time.sleep(2 ** attempt)  # Exponential backoff
                            continue
                    else:
                        # Final attempt failed
                        print(f"   âš ï¸ æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œä½¿ç”¨fallbackç»“æœ")
                        # Save the problematic response
                        debug_file = f"debug_json_final_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                        try:
                            with open(debug_file, 'w', encoding='utf-8') as f:
                                f.write(f"Final Error: {error_msg}\n")
                                f.write(f"Error position: {error_pos}\n")
                                f.write(f"Response length: {len(result_text)}\n")
                                f.write(f"Context: {result_text[max(0, error_pos-200):error_pos+200] if error_pos else 'N/A'}\n")
                                f.write(f"\nFull response:\n{result_text}\n")
                            print(f"   ğŸ’¾ å·²ä¿å­˜æœ€ç»ˆé”™è¯¯ä¿¡æ¯åˆ°: {debug_file}")
                        except:
                            pass
                        raise
                    
            except Exception as e:
                print(f"   âš ï¸ é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {str(e)[:100]}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                raise
        
        # Fallback if all retries failed
        return {
            "candidate_themes": [],
            "summary": "é”™è¯¯: é‡è¯•åä»æ— æ³•ç”Ÿæˆä¸»é¢˜",
            "total_comments": len(comments),
            "comments_analyzed": len(comments_to_process),
            "sample_info": comments_sample_info
        }
    
    def _clean_comment_for_json(self, comment: str, max_length: int = 500) -> str:
        """
        æ¸…æ´—è¯„è®ºå†…å®¹ï¼Œç¡®ä¿å¯ä»¥å®‰å…¨åœ°æ”¾å…¥JSONå­—ç¬¦ä¸²ä¸­
        
        Args:
            comment: åŸå§‹è¯„è®º
            max_length: æœ€å¤§é•¿åº¦ï¼Œè¶…è¿‡åˆ™æˆªæ–­
        
        Returns:
            æ¸…æ´—åçš„è¯„è®º
        """
        if not comment or not isinstance(comment, str):
            return ""
        
        # 1. ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆä¿ç•™æ¢è¡Œç¬¦ï¼Œç¨åå¤„ç†ï¼‰
        import re
        comment = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F]', '', comment)
        
        # 2. è½¬ä¹‰åæ–œæ ï¼ˆå¿…é¡»åœ¨è½¬ä¹‰å¼•å·ä¹‹å‰ï¼‰
        comment = comment.replace('\\', '\\\\')
        
        # 3. è½¬ä¹‰å¼•å·
        comment = comment.replace('"', '\\"')
        
        # 4. å°†æ¢è¡Œç¬¦è½¬æ¢ä¸º \nï¼ˆJSONæ ¼å¼ï¼‰
        comment = comment.replace('\n', '\\n').replace('\r', '')
        
        # 5. ç§»é™¤åˆ¶è¡¨ç¬¦æˆ–è½¬æ¢ä¸ºç©ºæ ¼
        comment = comment.replace('\t', ' ')
        
        # 6. é™åˆ¶é•¿åº¦ï¼ˆåœ¨å¥å­æˆ–å•è¯è¾¹ç•Œæˆªæ–­ï¼‰
        if len(comment) > max_length:
            # å°è¯•åœ¨å¥å·ã€æ„Ÿå¹å·ã€é—®å·åæˆªæ–­
            truncate_pos = max_length
            for punct in ['. ', '! ', '? ', 'ã€‚', 'ï¼', 'ï¼Ÿ']:
                last_pos = comment.rfind(punct, 0, max_length)
                if last_pos > 0:
                    truncate_pos = last_pos + len(punct)
                    break
            
            # å¦‚æœæ²¡æ‰¾åˆ°æ ‡ç‚¹ï¼Œå°è¯•åœ¨ç©ºæ ¼å¤„æˆªæ–­
            if truncate_pos == max_length:
                last_space = comment.rfind(' ', 0, max_length)
                if last_space > max_length * 0.8:  # è‡³å°‘ä¿ç•™80%çš„å†…å®¹
                    truncate_pos = last_space
            
            comment = comment[:truncate_pos] + "..."
        
        # 7. ç§»é™¤é¦–å°¾ç©ºç™½
        comment = comment.strip()
        
        return comment
    
    def generate_insights_sonnet(self, candidate_themes: Dict, module_name: str, 
                                 sentiment_name: str,
                                 comment_count: int, comments: List[str] = None, max_retries: int = 5) -> Dict:
        """
        Use Claude Sonnet 4.5 to generate insights from candidate themes
        
        Args:
            candidate_themes: Dictionary with candidate themes from Haiku
            module_name: Module name (category)
            sentiment_name: Sentiment name
            comment_count: Total number of comments
            comments: Original comments list for reference
            max_retries: Maximum number of retries for API calls
        
        Returns:
            Dictionary with insights
        """
        themes_text = json.dumps(candidate_themes, indent=2, ensure_ascii=False)
        
        # Include sample comments for reference (to ensure supporting_comments are accurate)
        # æ¸…æ´—è¯„è®ºå†…å®¹ï¼Œé¿å…JSONè§£æé”™è¯¯
        comments_ref = ""
        if comments:
            # Include up to 200 comments for reference (increased to allow more supporting_comments)
            sample_comments = comments[:200] if len(comments) > 200 else comments
            # æ¸…æ´—æ¯æ¡è¯„è®ºï¼Œç¡®ä¿å¯ä»¥å®‰å…¨åœ°æ”¾å…¥JSONï¼ˆä½†ä¸åœ¨promptä¸­è½¬ä¹‰ï¼Œè®©Claudeè‡ªå·±å¤„ç†ï¼‰
            # åªåšåŸºæœ¬æ¸…ç†ï¼šç§»é™¤æ§åˆ¶å­—ç¬¦ã€é™åˆ¶é•¿åº¦
            cleaned_comments = []
            for c in sample_comments:
                if not c or not isinstance(c, str):
                    continue
                # ç§»é™¤æ§åˆ¶å­—ç¬¦
                import re
                cleaned = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F]', '', str(c))
                # é™åˆ¶é•¿åº¦ï¼ˆåœ¨å•è¯è¾¹ç•Œæˆªæ–­ï¼‰
                if len(cleaned) > 400:
                    last_space = cleaned.rfind(' ', 0, 400)
                    if last_space > 300:
                        cleaned = cleaned[:last_space] + "..."
                    else:
                        cleaned = cleaned[:400] + "..."
                cleaned_comments.append(cleaned.strip())
            
            comments_ref = f"\n\nåŸå§‹è¯„è®ºæ ·æœ¬ï¼ˆä¾›å‚è€ƒï¼Œç¡®ä¿æ”¯æ’‘è¯„è®ºçš„å‡†ç¡®æ€§ï¼Œå…±{len(cleaned_comments)}æ¡ï¼‰ï¼š\n" + "\n".join([f"{i+1}. {c}" for i, c in enumerate(cleaned_comments)])
        
        for attempt in range(max_retries):
            try:
                prompt = f"""åŸºäºä»"{module_name}"ç±»åˆ«ã€æƒ…æ„Ÿä¸º"{sentiment_name}"çš„{comment_count}æ¡è¯„è®ºä¸­è¯†åˆ«å‡ºçš„å€™é€‰ä¸»é¢˜ï¼Œç”Ÿæˆæ·±å…¥çš„æ´å¯Ÿåˆ†æã€‚

å€™é€‰ä¸»é¢˜ï¼š
{themes_text}
{comments_ref}

è¦æ±‚ï¼š
1. å°†å€™é€‰ä¸»é¢˜ç»¼åˆæˆå…·ä½“çš„æ´å¯Ÿï¼ˆå¯ä»¥åŒ…æ‹¬è¡¨é¢è§‚å¯Ÿå’Œæ·±å±‚åˆ†æï¼‰
2. è¯†åˆ«æœ€é‡è¦çš„ä¸»é¢˜åŠå…¶å…·ä½“è¡¨ç°
3. çªå‡ºä»»ä½•ç´§æ€¥é—®é¢˜æˆ–æœºä¼š
4. è€ƒè™‘ä¸šåŠ¡èƒŒæ™¯ï¼šè¿™æ˜¯æ¸¸æˆäº§å“çš„ç”¨æˆ·åé¦ˆ
5. æ´å¯Ÿå¿…é¡»åŸºäºå®é™…çš„è¯„è®ºå†…å®¹ï¼Œè¦å…·ä½“ã€è¯¦ç»†
6. æ¯ä¸ªæ´å¯Ÿå¿…é¡»åŒ…å«æ”¯æ’‘è¯¥æ´å¯Ÿçš„å…·ä½“è¯„è®ºåŸæ–‡ä½œä¸ºè¯æ®
7. å…³é”®æ´å¯Ÿæ•°é‡ï¼šæ ¹æ®å€™é€‰ä¸»é¢˜æ•°é‡å’Œè¯„è®ºå†…å®¹çš„é‡è¦æ€§ï¼Œç”Ÿæˆ5-10ä¸ªå…³é”®æ´å¯Ÿã€‚å¦‚æœå€™é€‰ä¸»é¢˜æ•°é‡è¾ƒå¤šï¼ˆ>8ä¸ªï¼‰ï¼Œå¯ä»¥ç”Ÿæˆæ›´å¤šæ´å¯Ÿï¼›å¦‚æœå€™é€‰ä¸»é¢˜è¾ƒå°‘ï¼ˆ<5ä¸ªï¼‰ï¼Œå¯ä»¥é€‚å½“åˆå¹¶æˆ–èšç„¦æœ€é‡è¦çš„è¯é¢˜
8. ä¼˜å…ˆä¸»é¢˜æ•°é‡ï¼šä»å…³é”®æ´å¯Ÿä¸­è¯†åˆ«3-5ä¸ªæœ€é‡è¦çš„ä¼˜å…ˆä¸»é¢˜

è¾“å‡ºæ ¼å¼ï¼ˆä»…JSONï¼‰ï¼š
{{
  "key_insights": [
    {{
      "insight": "å…·ä½“çš„æ´å¯Ÿé™ˆè¿°ï¼ˆè¦è¯¦ç»†ã€å…·ä½“ï¼ŒåŸºäºå®é™…è¯„è®ºå†…å®¹ï¼‰",
      "importance": "é«˜|ä¸­|ä½",
      "supporting_comments": ["æ”¯æ’‘è¯¥æ´å¯Ÿçš„è¯„è®ºåŸæ–‡1", "æ”¯æ’‘è¯¥æ´å¯Ÿçš„è¯„è®ºåŸæ–‡2", "æ”¯æ’‘è¯¥æ´å¯Ÿçš„è¯„è®ºåŸæ–‡3", "æ”¯æ’‘è¯¥æ´å¯Ÿçš„è¯„è®ºåŸæ–‡4", "æ”¯æ’‘è¯¥æ´å¯Ÿçš„è¯„è®ºåŸæ–‡5", "..."]
    }}
  ],
  "priority_themes": [
    {{
      "theme_name": "ä¸»é¢˜åç§°",
      "why_important": "ä¸ºä»€ä¹ˆè¿™ä¸ªä¸»é¢˜é‡è¦ï¼ˆå…·ä½“åŸå› ï¼‰",
      "supporting_comments": ["æ”¯æ’‘è¯¥ä¸»é¢˜çš„è¯„è®ºåŸæ–‡1", "æ”¯æ’‘è¯¥ä¸»é¢˜çš„è¯„è®ºåŸæ–‡2", "æ”¯æ’‘è¯¥ä¸»é¢˜çš„è¯„è®ºåŸæ–‡3", "æ”¯æ’‘è¯¥ä¸»é¢˜çš„è¯„è®ºåŸæ–‡4", "æ”¯æ’‘è¯¥ä¸»é¢˜çš„è¯„è®ºåŸæ–‡5", "..."]
    }}
  ],
  "sentiment_analysis": {{
    "overall_sentiment": "æ­£é¢|è´Ÿé¢|ä¸­æ€§|æ··åˆ",
    "sentiment_explanation": "æƒ…æ„Ÿæ¨¡å¼çš„è§£é‡Šï¼ˆè¦å…·ä½“ï¼‰",
    "emotional_tone": "æƒ…æ„ŸåŸºè°ƒçš„æè¿°ï¼ˆè¦å…·ä½“ï¼‰",
    "supporting_comments": ["æ”¯æ’‘æƒ…æ„Ÿåˆ†æçš„è¯„è®ºåŸæ–‡1", "æ”¯æ’‘æƒ…æ„Ÿåˆ†æçš„è¯„è®ºåŸæ–‡2", "æ”¯æ’‘æƒ…æ„Ÿåˆ†æçš„è¯„è®ºåŸæ–‡3", "æ”¯æ’‘æƒ…æ„Ÿåˆ†æçš„è¯„è®ºåŸæ–‡4", "æ”¯æ’‘æƒ…æ„Ÿåˆ†æçš„è¯„è®ºåŸæ–‡5", "..."]
  }},
  "summary": "æ‰§è¡Œæ‘˜è¦ï¼ˆ3-5å¥è¯ï¼Œè¦å…·ä½“ï¼‰"
}}

é‡è¦æç¤ºï¼š
- æ´å¯Ÿå¿…é¡»å…·ä½“ã€è¯¦ç»†ï¼ŒåŸºäºå®é™…è¯„è®ºä¸­çš„å…·ä½“å†…å®¹å’Œè¡¨è¿°
- æ¯ä¸ªæ´å¯Ÿå’Œä¸»é¢˜éƒ½å¿…é¡»åŒ…å«æ”¯æ’‘è¯„è®ºçš„åŸæ–‡ä½œä¸ºè¯æ®ã€‚supporting_commentsä¸­çš„è¯„è®ºå¿…é¡»æ˜¯ä»åŸå§‹è¯„è®ºæ ·æœ¬ä¸­å‡†ç¡®å¼•ç”¨çš„åŸæ–‡ï¼Œä¸èƒ½æ”¹å†™æˆ–æ€»ç»“
- supporting_commentsæ•°é‡è¦æ±‚ï¼š
  * å…³é”®æ´å¯Ÿï¼ˆkey_insightsï¼‰ï¼šæ¯ä¸ªæ´å¯Ÿåº”è¯¥åŒ…å«5-10æ¡æ”¯æ’‘è¯„è®ºåŸæ–‡ï¼Œå¦‚æœè¯¥æ´å¯Ÿæœ‰å¾ˆå¤šç›¸å…³è¯„è®ºï¼Œå¯ä»¥åŒ…å«æ›´å¤šï¼ˆæœ€å¤š15æ¡ï¼‰
  * ä¼˜å…ˆä¸»é¢˜ï¼ˆpriority_themesï¼‰ï¼šæ¯ä¸ªä¸»é¢˜åº”è¯¥åŒ…å«5-8æ¡æ”¯æ’‘è¯„è®ºåŸæ–‡ï¼Œå¦‚æœè¯¥ä¸»é¢˜æœ‰å¾ˆå¤šç›¸å…³è¯„è®ºï¼Œå¯ä»¥åŒ…å«æ›´å¤šï¼ˆæœ€å¤š12æ¡ï¼‰
  * æƒ…æ„Ÿåˆ†æï¼ˆsentiment_analysisï¼‰ï¼šåº”è¯¥åŒ…å«5-8æ¡æ”¯æ’‘è¯„è®ºåŸæ–‡ï¼Œå±•ç¤ºä¸åŒæƒ…æ„Ÿçš„è¯„è®ºç¤ºä¾‹
- é€‰æ‹©æ”¯æ’‘è¯„è®ºæ—¶ï¼Œåº”è¯¥é€‰æ‹©æœ€èƒ½ä»£è¡¨è¯¥æ´å¯Ÿ/ä¸»é¢˜çš„è¯„è®ºï¼Œä¼˜å…ˆé€‰æ‹©ç‚¹èµæ•°è¾ƒé«˜ã€è¡¨è¿°æ¸…æ™°çš„è¯„è®º
- å¦‚æœæŸä¸ªæ´å¯Ÿæˆ–ä¸»é¢˜çš„ç›¸å…³è¯„è®ºå¾ˆå¤šï¼Œä¸è¦åªé€‰æ‹©3-4æ¡ï¼Œåº”è¯¥åŒ…å«æ›´å¤šæ”¯æ’‘è¯„è®ºä»¥å……åˆ†è¯æ˜è¯¥æ´å¯Ÿ
- ä¸è¦æä¾›recommendationæˆ–implicationå­—æ®µ
- æ´å¯Ÿå¯ä»¥åŒ…æ‹¬è¡¨é¢è§‚å¯Ÿå’Œæ·±å±‚åˆ†æï¼Œåªè¦æ˜¯åŸºäºå®é™…è¯„è®ºå†…å®¹çš„çœŸå®æ´å¯Ÿå³å¯
- ä»…è¿”å›æœ‰æ•ˆçš„JSONï¼Œä¸è¦markdownä»£ç å—ï¼Œä¸è¦è§£é‡Š
- é‡è¦ï¼šæ‰€æœ‰å­—ç¬¦ä¸²å€¼ä¸­çš„å¼•å·ã€æ¢è¡Œç¬¦ã€åæ–œæ ç­‰ç‰¹æ®Šå­—ç¬¦å¿…é¡»æ­£ç¡®è½¬ä¹‰ï¼ˆä½¿ç”¨\\\"è¡¨ç¤ºå¼•å·ï¼Œ\\nè¡¨ç¤ºæ¢è¡Œï¼Œ\\\\è¡¨ç¤ºåæ–œæ ï¼‰
- é‡è¦ï¼šç¡®ä¿æ‰€æœ‰JSONå­—ç¬¦ä¸²éƒ½æ­£ç¡®å…³é—­ï¼Œä¸è¦æˆªæ–­ä»»ä½•å­—ç¬¦ä¸²å€¼ã€‚å¦‚æœæŸä¸ªå­—æ®µçš„å€¼å¾ˆé•¿ï¼Œå¯ä»¥é€‚å½“ç¼©çŸ­ï¼Œä½†å¿…é¡»ç¡®ä¿JSONç»“æ„å®Œæ•´æœ‰æ•ˆ
- é‡è¦ï¼šsupporting_commentsä¸­çš„è¯„è®ºåŸæ–‡åº”è¯¥ä»ä¸Šé¢æä¾›çš„"åŸå§‹è¯„è®ºæ ·æœ¬"ä¸­å‡†ç¡®å¼•ç”¨ï¼Œå¼•ç”¨æ—¶ä¿æŒåŸæ–‡çš„è½¬ä¹‰æ ¼å¼ï¼ˆå¼•å·å·²è½¬ä¹‰ä¸º\\\"ï¼Œæ¢è¡Œå·²è½¬ä¹‰ä¸º\\nï¼‰"""

                response = self.anthropic_client.messages.create(
                    model=self.sonnet_model,
                    max_tokens=8192,  # Large limit to allow full responses
                    messages=[
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ]
                )
                
                result_text = response.content[0].text.strip()
                
                # Remove markdown code blocks if present
                if result_text.startswith("```"):
                    result_text = result_text.split("```")[1]
                    if result_text.startswith("json"):
                        result_text = result_text[4:]
                result_text = result_text.strip()
                
                # Try to fix common JSON issues
                result_text = self._fix_json_text(result_text)
                
                # Parse JSON
                try:
                    result = json.loads(result_text)
                    # Ensure all insights have supporting_comments
                    for insight in result.get('key_insights', []):
                        if 'supporting_comments' not in insight:
                            insight['supporting_comments'] = []
                    for theme in result.get('priority_themes', []):
                        if 'supporting_comments' not in theme:
                            theme['supporting_comments'] = []
                    if 'sentiment_analysis' in result and 'supporting_comments' not in result['sentiment_analysis']:
                        result['sentiment_analysis']['supporting_comments'] = []
                    return result
                except json.JSONDecodeError as e:
                    error_msg = str(e)
                    error_pos = getattr(e, 'pos', None)
                    print(f"   âš ï¸ JSONè§£æé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {error_msg[:150]}")
                    if error_pos:
                        print(f"      é”™è¯¯ä½ç½®: {error_pos}, ä¸Šä¸‹æ–‡: {result_text[max(0, error_pos-50):error_pos+50]}")
                    
                    if attempt < max_retries - 1:
                        # Try more aggressive JSON fixing
                        print(f"   ğŸ”„ å°è¯•æ›´æ¿€è¿›çš„JSONä¿®å¤...")
                        result_text = self._fix_json_text(result_text)
                        
                        # Try parsing again with fixed text
                        try:
                            result = json.loads(result_text)
                            # If successful, continue with the fixed result
                            print(f"   âœ… JSONä¿®å¤æˆåŠŸï¼")
                            # Ensure all insights have supporting_comments
                            for insight in result.get('key_insights', []):
                                if 'supporting_comments' not in insight:
                                    insight['supporting_comments'] = []
                            for theme in result.get('priority_themes', []):
                                if 'supporting_comments' not in theme:
                                    theme['supporting_comments'] = []
                            if 'sentiment_analysis' in result and 'supporting_comments' not in result['sentiment_analysis']:
                                result['sentiment_analysis']['supporting_comments'] = []
                            return result
                        except json.JSONDecodeError:
                            # Still failing, retry with new API call
                            print(f"   ğŸ”„ JSONä¿®å¤å¤±è´¥ï¼Œé‡è¯•APIè°ƒç”¨ (ç­‰å¾… {2 ** attempt} ç§’)...")
                            # Save problematic response for debugging
                            if attempt == 1:
                                debug_file = f"debug_json_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                                try:
                                    with open(debug_file, 'w', encoding='utf-8') as f:
                                        f.write(f"Error: {error_msg}\n")
                                        f.write(f"Error position: {error_pos}\n")
                                        f.write(f"Response length: {len(result_text)}\n")
                                        f.write(f"Context: {result_text[max(0, error_pos-100):error_pos+100] if error_pos else 'N/A'}\n")
                                        f.write(f"\nFull response:\n{result_text}\n")
                                    print(f"   ğŸ’¾ å·²ä¿å­˜è°ƒè¯•ä¿¡æ¯åˆ°: {debug_file}")
                                except:
                                    pass
                            time.sleep(2 ** attempt)  # Exponential backoff
                            continue
                    else:
                        # Final attempt failed
                        print(f"   âš ï¸ æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œä½¿ç”¨fallbackç»“æœ")
                        # Save the problematic response
                        debug_file = f"debug_json_final_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                        try:
                            with open(debug_file, 'w', encoding='utf-8') as f:
                                f.write(f"Final Error: {error_msg}\n")
                                f.write(f"Error position: {error_pos}\n")
                                f.write(f"Response length: {len(result_text)}\n")
                                f.write(f"Context: {result_text[max(0, error_pos-200):error_pos+200] if error_pos else 'N/A'}\n")
                                f.write(f"\nFull response:\n{result_text}\n")
                            print(f"   ğŸ’¾ å·²ä¿å­˜æœ€ç»ˆé”™è¯¯ä¿¡æ¯åˆ°: {debug_file}")
                        except:
                            pass
                        raise
                    
            except Exception as e:
                print(f"   âš ï¸ é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {str(e)[:100]}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                raise
        
        # Fallback if all retries failed
        return {
            "key_insights": [],
            "priority_themes": [],
            "sentiment_analysis": {
                "overall_sentiment": sentiment_name,
                "sentiment_explanation": "é”™è¯¯: æ— æ³•ç”Ÿæˆæ´å¯Ÿ",
                "emotional_tone": "æœªçŸ¥",
                "supporting_comments": []
            },
            "summary": "é”™è¯¯: é‡è¯•åä»æ— æ³•ç”Ÿæˆæ´å¯Ÿ"
        }
    
    def process_all_groups(self):
        """Process all comment groups: map themes with Haiku, then generate insights with Sonnet"""
        print(f"\nğŸ¤– ä½¿ç”¨ Claude AI å¤„ç† {len(self.grouped_comments)} ä¸ªç»„...")
        print(f"   æ­¥éª¤ 1: ä½¿ç”¨ Claude Haiku 4.5 ç”Ÿæˆå€™é€‰ä¸»é¢˜")
        print(f"   æ­¥éª¤ 2: ä½¿ç”¨ Claude Sonnet 4.5 ç”Ÿæˆæ´å¯Ÿ")
        
        # Sort groups by comment count (process larger groups first)
        sorted_groups = sorted(self.grouped_comments.items(), 
                             key=lambda x: x[1]['comment_count'], 
                             reverse=True)
        
        # Apply limit if specified (for testing)
        # Also support filtering by group key pattern
        if self.limit_groups:
            if isinstance(self.limit_groups, str):
                # Filter by group key pattern (e.g., "Monetization|Positive")
                sorted_groups = [(k, v) for k, v in sorted_groups if self.limit_groups in k]
                print(f"   âš ï¸  ç­›é€‰ç»„: åŒ…å« '{self.limit_groups}'")
            elif isinstance(self.limit_groups, int) and self.limit_groups > 0:
                sorted_groups = sorted_groups[:self.limit_groups]
                print(f"   âš ï¸  é™åˆ¶ä¸º {self.limit_groups} ä¸ªç»„è¿›è¡Œæµ‹è¯•")
        
        self.candidate_themes = {}
        self.insights = {}
        
        # Process each group
        for idx, (group_key, group_data) in enumerate(tqdm(sorted_groups, desc="   å¤„ç†ç»„")):
            module_name = group_data['module_name']
            sentiment_name = group_data['sentiment_name']
            comments = group_data['comments']
            comment_count = group_data['comment_count']
            
            print(f"\n   [{idx+1}/{len(sorted_groups)}] å¤„ç†ä¸­: {group_key} ({comment_count} æ¡è¯„è®º)")
            
            # Step 1: Map candidate themes with Haiku
            try:
                candidate_themes = self.map_candidate_themes_haiku(
                    comments, module_name, sentiment_name
                )
                self.candidate_themes[group_key] = candidate_themes
                print(f"      âœ… ç”Ÿæˆäº† {len(candidate_themes.get('candidate_themes', []))} ä¸ªå€™é€‰ä¸»é¢˜")
            except Exception as e:
                print(f"      âŒ ç”Ÿæˆä¸»é¢˜æ—¶å‡ºé”™: {str(e)[:100]}")
                self.candidate_themes[group_key] = {
                    "candidate_themes": [],
                    "summary": f"Error: {str(e)}",
                    "total_comments": comment_count
                }
                continue
            
            # Step 2: Generate insights with Sonnet
            try:
                insights = self.generate_insights_sonnet(
                    candidate_themes, module_name, sentiment_name, comment_count, comments
                )
                self.insights[group_key] = insights
                print(f"      âœ… ç”Ÿæˆäº†æ´å¯Ÿ")
            except Exception as e:
                print(f"      âŒ Error generating insights: {str(e)[:100]}")
                self.insights[group_key] = {
                    "key_insights": [],
                    "priority_themes": [],
                    "sentiment_analysis": {
                        "overall_sentiment": sentiment_name,
                        "sentiment_explanation": f"é”™è¯¯: {str(e)}",
                        "emotional_tone": "æœªçŸ¥",
                        "supporting_comments": []
                    },
                    "summary": f"é”™è¯¯: {str(e)}"
                }
                continue
            
            # Rate limiting: small delay between groups
            time.sleep(1)
        
        print(f"\n   âœ… å·²å¤„ç† {len(self.candidate_themes)} ä¸ªç»„")
    
    def save_results(self):
        """Save results to files"""
        print(f"\nğŸ’¾ Saving results...")
        
        # Create output directory if needed
        output_dir = self.output_dir or ""
        if output_dir and not output_dir.endswith('/'):
            output_dir += '/'
        
        # Save candidate themes
        themes_file = f"{output_dir}candidate_themes_{self.timestamp}.json"
        with open(themes_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'timestamp': self.timestamp,
                    'total_groups': len(self.candidate_themes),
                    'source_file': self.processed_csv_file
                },
                'groups': self.candidate_themes
            }, f, indent=2, ensure_ascii=False)
        print(f"   â€¢ {themes_file}")
        
        # Save insights
        insights_file = f"{output_dir}insights_{self.timestamp}.json"
        with open(insights_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'timestamp': self.timestamp,
                    'total_groups': len(self.insights),
                    'source_file': self.processed_csv_file
                },
                'groups': self.insights
            }, f, indent=2, ensure_ascii=False)
        print(f"   â€¢ {insights_file}")
        
        # Save combined results (themes + insights + group data)
        combined_file = f"{output_dir}theme_insights_combined_{self.timestamp}.json"
        combined_data = {
            'metadata': {
                'timestamp': self.timestamp,
                'total_groups': len(self.grouped_comments),
                'source_file': self.processed_csv_file
            },
            'groups': {}
        }
        
        for group_key in self.grouped_comments.keys():
            combined_data['groups'][group_key] = {
                'group_info': self.grouped_comments[group_key],
                'candidate_themes': self.candidate_themes.get(group_key, {}),
                'insights': self.insights.get(group_key, {})
            }
        
        with open(combined_file, 'w', encoding='utf-8') as f:
            json.dump(combined_data, f, indent=2, ensure_ascii=False)
        print(f"   â€¢ {combined_file}")
        
        # Save summary CSV
        summary_file = f"{output_dir}insights_summary_{self.timestamp}.csv"
        summary_rows = []
        
        for group_key, group_data in self.grouped_comments.items():
            insights_data = self.insights.get(group_key, {})
            themes_data = self.candidate_themes.get(group_key, {})
            
            row = {
                'module_name': group_data['module_name'],
                'sentiment_name': group_data['sentiment_name'],
                'comment_count': group_data['comment_count'],
                'like_count': group_data['like_count'],
                'avg_likes': group_data['avg_likes'],
                'num_candidate_themes': len(themes_data.get('candidate_themes', [])),
                'num_key_insights': len(insights_data.get('key_insights', [])),
                'num_priority_themes': len(insights_data.get('priority_themes', [])),
                'overall_sentiment': insights_data.get('sentiment_analysis', {}).get('overall_sentiment', ''),
                'summary': insights_data.get('summary', '')[:500] if insights_data.get('summary') else ''  # Limit summary length for CSV
            }
            summary_rows.append(row)
        
        summary_df = pd.DataFrame(summary_rows)
        summary_df = summary_df.sort_values('comment_count', ascending=False)
        summary_df.to_csv(summary_file, index=False, encoding='utf-8-sig')
        print(f"   â€¢ {summary_file}")
        
        return themes_file, insights_file, combined_file, summary_file
    
    def run_full_analysis(self):
        """Run the complete analysis pipeline"""
        print("ğŸ¬ Theme Insight Generator")
        print("=" * 50)
        print(f"âš™ï¸ Configuration:")
        print(f"   Model (Map): Claude Haiku 4.5")
        print(f"   Model (Reduce): Claude Sonnet 4.5")
        print(f"   Source file: {self.processed_csv_file}")
        
        # Load data
        self.load_config()
        self.load_data()
        
        # Aggregate comments
        self.aggregate_comments()
        
        # Process all groups
        self.process_all_groups()
        
        # Save results
        output_files = self.save_results()
        
        # Print summary
        print("\n" + "=" * 50)
        print("ğŸ‰ ANALYSIS COMPLETE")
        print("=" * 50)
        print(f"\nğŸ“Š Summary:")
        print(f"   Total groups processed: {len(self.grouped_comments)}")
        print(f"   Groups with themes: {len(self.candidate_themes)}")
        print(f"   Groups with insights: {len(self.insights)}")
        
        print(f"\nğŸ“ Output files:")
        for output_file in output_files:
            print(f"   â€¢ {output_file}")


def main():
    import argparse
    
    ap = argparse.ArgumentParser(
        description="Generate theme insights from processed comments using Claude AI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Process comments with API key from environment
  python theme_insight_generator.py comments_processed_20251110_105015.csv
  
  # Process with explicit API key
  python theme_insight_generator.py comments_processed_20251110_105015.csv --api-key YOUR_API_KEY
  
  # Specify output directory
  python theme_insight_generator.py comments_processed_20251110_105015.csv --output-dir results/
        """
    )
    
    ap.add_argument("processed_csv", help="Path to processed comments CSV file")
    ap.add_argument("--config", type=str, default="themes_config_new.yaml", 
                   help="Theme configuration YAML file")
    ap.add_argument("--api-key", type=str, default=None, 
                   help="Anthropic API key (or use ANTHROPIC_API_KEY env var)")
    ap.add_argument("--output-dir", type=str, default=None, 
                   help="Output directory for results")
    ap.add_argument("--limit-groups", type=str, default=None,
                   help="Limit number of groups to process (integer) or filter by group pattern (string, e.g., 'Monetization|Positive')")
    
    args = ap.parse_args()
    
    # Check if file exists
    if not os.path.exists(args.processed_csv):
        print(f"âŒ Error: File not found: {args.processed_csv}")
        return
    
    # Initialize and run
    generator = ThemeInsightGenerator(
        processed_csv_file=args.processed_csv,
        config_file=args.config,
        api_key=args.api_key,
        output_dir=args.output_dir
    )
    
    # Add limit_groups attribute if specified
    if args.limit_groups:
        # Try to parse as integer first
        try:
            generator.limit_groups = int(args.limit_groups)
        except ValueError:
            # If not an integer, treat as string pattern
            generator.limit_groups = args.limit_groups
    else:
        generator.limit_groups = None
    
    generator.run_full_analysis()


if __name__ == "__main__":
    main()

